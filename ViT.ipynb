{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setting Seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シード値の設定\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Masked AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attentionの実装\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．埋め込み次元数と一致する．\n",
    "        heads : int\n",
    "            ヘッドの数．\n",
    "        dim_head : int\n",
    "            各ヘッドのデータの次元数．\n",
    "        dropout : float\n",
    "            Dropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n",
    "        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Q, K, Vに変換するための全結合層\n",
    "        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "\n",
    "        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(in_features=inner_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        D: データの次元数(dim)\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "\n",
    "        # 入力データをQ, K, Vに変換する\n",
    "        # (B, N, dim) -> (B, N, inner_dim)\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # Q, K, Vをヘッドに分割する\n",
    "        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # QK^T / sqrt(d_k)を計算する\n",
    "        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n",
    "        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # ソフトマックス関数でスコアを算出し，Dropoutをする\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # softmax(QK^T / sqrt(d_k))Vを計算する\n",
    "        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n",
    "        out = torch.matmul(attn ,v)\n",
    "\n",
    "        # もとの形に戻す\n",
    "        # (B, heads, N, dim_head) -> (B, N, dim)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # 次元が違っていればもとに戻して出力\n",
    "        # 表現の可視化のためにattention mapも返すようにしておく\n",
    "        return self.to_out(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 384])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(384, 12, 32, 0.)\n",
    "x = torch.rand(4, 64, 384)  # (B, N, D)\n",
    "attn(x)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Networkの実装\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．\n",
    "        hidden_dim : int\n",
    "            隠れ層の次元．\n",
    "        dropout : float\n",
    "            各全結合層の後のDropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        (B, D) -> (B, D)\n",
    "        B: バッチサイズ\n",
    "        D: 次元数\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = FFN(384, 768, 0.)\n",
    "x = torch.rand(4, 64, 384)  # (B, N, D)\n",
    "ffn(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Transformer BlocK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n",
    "        \"\"\"\n",
    "        TransformerのEncoder Blockの実装．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        dropout : float\n",
    "            Droptou層の確率p．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n",
    "        self.attn = Attention(dim, heads, dim_head, dropout)\n",
    "        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n",
    "        self.ffn = FFN(dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        \"\"\"\n",
    "        x: (B, N, dim)\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        dim: 埋め込み次元\n",
    "        \"\"\"\n",
    "        y, attn = self.attn(self.attn_ln(x))\n",
    "        if return_attn:  # attention mapを返す（attention mapの可視化に利用）\n",
    "            return attn\n",
    "        x = y + x\n",
    "        out = self.ffn(self.ffn_ln(x)) + x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = Block(384, 12, 32, 384, 0.)\n",
    "x = torch.rand(4, 64, 384)  # (B, N, D)\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embeddingの実装\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        入力画像をパッチごとに埋め込むための層．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        image_size : Tuple[int]\n",
    "            入力画像のサイズ．\n",
    "        patch_size : Tuple[int]\n",
    "            各パッチのサイズ．\n",
    "        in_channels : int\n",
    "            入力画像のチャネル数．\n",
    "        embed_dim : int\n",
    "            埋め込み後の次元数．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n",
    "        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n",
    "            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        C: 入力画像のチャネル数\n",
    "        H: 入力画像の高さ\n",
    "        W: 入力画像の幅\n",
    "        \"\"\"\n",
    "        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6400, 384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_emb = PatchEmbedding((240, 240), (3, 3), 1, 384)\n",
    "x = torch.rand((4, 1, 240, 240))  # (B, C, H, W)\n",
    "patch_emb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Masked AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_indexes(size):\n",
    "    \"\"\"\n",
    "    パッチをランダムに並べ替えるためのindexを生成する関数．\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "    size : int\n",
    "        入力されるパッチの数（系列長Nと同じ値）．\n",
    "    \"\"\"\n",
    "    original_indexes = np.arange(size)  # 0からsizeまでを並べた配列を作成\n",
    "    idx = np.random.randint(0,9)\n",
    "    forward_indexes = np.delete(original_indexes,idx) #delete element whose index is idx\n",
    "    forward_indexes = np.append(forward_indexes,idx)\n",
    "    #np.random.shuffle(original_indexes)  # 生成した配列をシャッフルすることで，パッチの順番をランダムに決定\n",
    "    #backward_indexes = np.argsort(original_indexes)  # 並べ替えたパッチをもとの順番に戻すためのidx\n",
    "\n",
    "    return forward_indexes, original_indexes#,backward_indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For patched imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_indexes(sequences, indexes):\n",
    "    \"\"\"\n",
    "    パッチを並べ替えるための関数．\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "    sequences : torch.Tensor\n",
    "        入力画像をパッチ分割したデータ．(B, N, dim)の形状をしている．\n",
    "    indexes : np.ndarray\n",
    "        並べ替えるために利用するindex．\n",
    "        random_indexesで生成したforward_indexesかbackward_indexesが入ることが想定されている．\n",
    "    \"\"\"\n",
    "    return torch.gather(sequences, dim=1, index=indexes.unsqueeze(2).repeat(1, 1, sequences.shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Patch Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchShuffle(nn.Module):\n",
    "    def __init__(self, ratio):\n",
    "        # ratio: Encoderに入力しないパッチの割合\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        N: 系列長（＝パッチの数）\n",
    "        dim: 次元数（＝埋め込みの次元数）\n",
    "        \"\"\"\n",
    "        B, N, dim = patches.shape\n",
    "        remain_N = int(N * (1 - self.ratio))  # Encoderに入力するパッチの数\n",
    "\n",
    "        indexes = [random_indexes(N) for _ in range(B)]  # バッチごとに異なる順番のindexを作る\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # バッチを並べ替えるときのidx (B, N)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).T.to(patches.device)  # 並べ替えたパッチをもとの順番に戻すためのidx  (B, N)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)  # パッチを並べ替える\n",
    "        patches = patches[:, :remain_N, :]  # Encoderに入力するパッチを抽出\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: shape torch.Size([4, 9, 2]) \n",
      " tensor([[[0.6623, 0.1954],\n",
      "         [0.6673, 0.0999],\n",
      "         [0.1798, 0.1487],\n",
      "         [0.0133, 0.7968],\n",
      "         [0.2802, 0.3515],\n",
      "         [0.0869, 0.7103],\n",
      "         [0.5998, 0.5146],\n",
      "         [0.3150, 0.8660],\n",
      "         [0.4400, 0.9140]],\n",
      "\n",
      "        [[0.4125, 0.4153],\n",
      "         [0.5973, 0.1560],\n",
      "         [0.9578, 0.9554],\n",
      "         [0.4740, 0.6574],\n",
      "         [0.9723, 0.5430],\n",
      "         [0.9235, 0.6739],\n",
      "         [0.8081, 0.6598],\n",
      "         [0.4666, 0.3855],\n",
      "         [0.2809, 0.0142]],\n",
      "\n",
      "        [[0.2417, 0.3581],\n",
      "         [0.8584, 0.7293],\n",
      "         [0.4957, 0.7993],\n",
      "         [0.5791, 0.8577],\n",
      "         [0.7174, 0.8722],\n",
      "         [0.1950, 0.1479],\n",
      "         [0.0028, 0.5691],\n",
      "         [0.9054, 0.2978],\n",
      "         [0.4905, 0.7861]],\n",
      "\n",
      "        [[0.4056, 0.8240],\n",
      "         [0.6851, 0.4904],\n",
      "         [0.5218, 0.9017],\n",
      "         [0.0588, 0.4897],\n",
      "         [0.4602, 0.4898],\n",
      "         [0.0048, 0.1515],\n",
      "         [0.2155, 0.7345],\n",
      "         [0.3676, 0.9367],\n",
      "         [0.5758, 0.8943]]])\n",
      "encoder input data: shape torch.Size([4, 8, 2]) \n",
      " tensor([[[0.6623, 0.1954],\n",
      "         [0.6673, 0.0999],\n",
      "         [0.0133, 0.7968],\n",
      "         [0.2802, 0.3515],\n",
      "         [0.0869, 0.7103],\n",
      "         [0.5998, 0.5146],\n",
      "         [0.3150, 0.8660],\n",
      "         [0.4400, 0.9140]],\n",
      "\n",
      "        [[0.4125, 0.4153],\n",
      "         [0.5973, 0.1560],\n",
      "         [0.9578, 0.9554],\n",
      "         [0.4740, 0.6574],\n",
      "         [0.9235, 0.6739],\n",
      "         [0.8081, 0.6598],\n",
      "         [0.4666, 0.3855],\n",
      "         [0.2809, 0.0142]],\n",
      "\n",
      "        [[0.2417, 0.3581],\n",
      "         [0.8584, 0.7293],\n",
      "         [0.5791, 0.8577],\n",
      "         [0.7174, 0.8722],\n",
      "         [0.1950, 0.1479],\n",
      "         [0.0028, 0.5691],\n",
      "         [0.9054, 0.2978],\n",
      "         [0.4905, 0.7861]],\n",
      "\n",
      "        [[0.4056, 0.8240],\n",
      "         [0.6851, 0.4904],\n",
      "         [0.5218, 0.9017],\n",
      "         [0.4602, 0.4898],\n",
      "         [0.0048, 0.1515],\n",
      "         [0.2155, 0.7345],\n",
      "         [0.3676, 0.9367],\n",
      "         [0.5758, 0.8943]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((4, 9, 2))  # (B, N, dim) 見やすいように小さい行列にしている\n",
    "patch_shuffle = PatchShuffle(ratio=1/9)\n",
    "in_patches, forward_idx, backward_idx = patch_shuffle(x)\n",
    "\n",
    "print(f\"original data: shape {x.shape} \\n {x}\")\n",
    "print(f\"encoder input data: shape {in_patches.shape} \\n {in_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self, image_size=[240, 240], patch_size=[3, 3], emb_dim=192, num_layer=12,\n",
    "                 heads=3, dim_head=64, mlp_dim=192, mask_ratio=1/9, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        num_layer : int\n",
    "            Encoderに含まれるBlockの数．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        mask_ratio : float\n",
    "            入力パッチのマスクする割合．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_height, img_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        # 入力画像をパッチに分割する\n",
    "        self.patchify = PatchEmbedding(image_size, patch_size, 1, emb_dim) #Gray scale img\n",
    "\n",
    "        # Encoder（Blockを重ねる）\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # 1. 入力画像をパッチに分割して，positional embeddingする\n",
    "        patches = self.patchify(img)\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        # 2. 分割したパッチをランダムに並べ替えて，必要なパッチのみ得る\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        # class tokenを結合\n",
    "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n",
    "\n",
    "        # 3. Encoderで入力データを処理する\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "\n",
    "        return features, backward_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MAE_Encoder()\n",
    "x = torch.rand((512, 1, 240,240))  # (B, N, dim) 見やすいように小さい行列にしている\n",
    "features, backward_indexes = encoder(x)\n",
    "print(features.shape)\n",
    "print(backward_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Decoder(nn.Module):\n",
    "    def __init__(self, image_size=[240, 240], patch_size=[3, 3], emb_dim=192, num_layer=4,\n",
    "                 heads=3, dim_head=64, mlp_dim=192, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        num_layer : int\n",
    "            Decoderに含まれるBlockの数．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_height, img_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.rand(1, num_patches+1, emb_dim))\n",
    "\n",
    "        # Decoder(Blockを重ねる）\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
    "\n",
    "        # 埋め込みされた表現から画像を復元するためのhead\n",
    "        self.head = torch.nn.Linear(emb_dim, patch_height * patch_width)\n",
    "        # (B, N, dim)から(B, C, H, W)にreshapeするためのインスタンス\n",
    "        self.patch2img = Rearrange(\"b (h w) (c p1 p2) -> b c (h p1) (w p2)\", p1=patch_height, p2=patch_width, h=img_height // patch_height)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        # 系列長\n",
    "        T = features.shape[1]\n",
    "\n",
    "        # class tokenがある分backward_indexesの最初に0を追加する\n",
    "        # .toはデバイスの変更でよく利用するが，tensorを渡すことでdtypeを変えることができる\n",
    "        backward_indexes = torch.cat([torch.zeros(backward_indexes.shape[0], 1).to(backward_indexes), backward_indexes+1], dim=1)\n",
    "\n",
    "        # 1. mask_tokenを結合して並べ替える．\n",
    "        # (B, N*(1-mask_ratio)+1, dim) -> (B, N+1, dim)\n",
    "        features = torch.cat([features, self.mask_token.repeat(features.shape[0], backward_indexes.shape[1] - features.shape[1], 1)], dim=1)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = self.transformer(features)\n",
    "\n",
    "        # class tokenを除去する\n",
    "        # (B, N+1, dim) -> (B, N, dim)\n",
    "        features = features[:, 1:, :]\n",
    "\n",
    "        # 2. 画像を再構成する．\n",
    "        # (B, N, dim) -> (B, N, 3 * patch_height * patch_width)\n",
    "        patches = self.head(features)\n",
    "\n",
    "        # MAEではマスクした部分でのみ損失関数を計算するため，maskも一緒に返す\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[:, T-1:] = 1  # cls tokenを含めていた分ずらしている\n",
    "        mask = take_indexes(mask, backward_indexes[:, 1:] - 1)\n",
    "\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Masked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self, image_size=[240, 240], patch_size=[3, 3], emb_dim=192,\n",
    "                 enc_layers=12, enc_heads=3, enc_dim_head=64, enc_mlp_dim=768,\n",
    "                 dec_layers=4, dec_heads=3, dec_dim_head=64, dec_mlp_dim=768,\n",
    "                 mask_ratio=1/9, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        {enc/dec}_layers : int\n",
    "            Encoder / Decoderに含まれるBlockの数．\n",
    "        {enc/dec}_heads : int\n",
    "            Encoder / DecoderのMulti-Head Attentionのヘッドの数．\n",
    "        {enc/dec}_dim_head : int\n",
    "            Encoder / DecoderのMulti-Head Attentionの各ヘッドの次元数．\n",
    "        {enc/dec}_mlp_dim : int\n",
    "            Encoder / DecoderのFeed-Forward Networkの隠れ層の次元数．\n",
    "        mask_ratio : float\n",
    "            入力パッチのマスクする割合．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, enc_layers,\n",
    "                                   enc_heads, enc_dim_head, enc_mlp_dim, mask_ratio, dropout)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, dec_layers,\n",
    "                                   dec_heads, dec_dim_head, dec_mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        rec_img, mask = self.decoder(features, backward_indexes)\n",
    "        return rec_img, mask\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        patches = self.encoder.patchify(x)\n",
    "        patches = patches + self.encoder.pos_embedding\n",
    "\n",
    "        patches = torch.cat([self.encoder.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
    "        for i, block in enumerate(self.encoder.transformer):\n",
    "            if i < len(self.encoder.transformer) - 1:\n",
    "                patches = block(patches)\n",
    "            else:\n",
    "                return block(patches, return_attn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cosine Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine scheduler\n",
    "class CosineScheduler:\n",
    "    def __init__(self, epochs, lr, warmup_length):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epochs : int\n",
    "            学習のエポック数．\n",
    "        lr : float\n",
    "            学習率．\n",
    "        warmup_length : int\n",
    "            warmupを適用するエポック数．\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup_length\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epoch : int\n",
    "            現在のエポック数．\n",
    "        \"\"\"\n",
    "        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n",
    "        progress = np.clip(progress, 0.0, 1.0)\n",
    "        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n",
    "\n",
    "        if self.warmup:\n",
    "            lr = lr * min(1., (epoch+1) / self.warmup)\n",
    "\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scheduler = CosineScheduler(epochs=1000, lr=0.01,warmup_length=100)\n",
    "x = np.arange(1000)\n",
    "plt.plot(x, [scheduler(epoch) for epoch in x])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LR\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "config = {\n",
    "    \"image_size\": [120, 120],\n",
    "    \"patch_size\": [3, 3],\n",
    "    \"emb_dim\": 192,\n",
    "    \"enc_layers\": 12,\n",
    "    \"enc_heads\": 3,\n",
    "    \"enc_dim_head\": 64,\n",
    "    \"enc_mlp_dim\": 192,\n",
    "    \"dec_layers\": 4,\n",
    "    \"dec_heads\": 3,\n",
    "    \"dec_dim_head\": 64,\n",
    "    \"dec_mlp_dim\": 192,\n",
    "    \"mask_ratio\": 1/9,\n",
    "    \"dropout\": 0.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = MAE_ViT(**config).to(device)\n",
    "\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "warmup_length = 20\n",
    "batch_size = 512\n",
    "step_count = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\n",
    "scheduler = CosineScheduler(epochs, lr, warmup_length=warmup_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "\"\"\"\n",
    "SRC_TRAIN = \"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20230302/dataset/train_vit_new2old.csv\"\n",
    "SRC_TEST = \"/content/gdrive/My Drive/YAMAKAWA_LAB/技術補佐員/動画/20230302/dataset/test_vit_new2old.csv\"\n",
    "\"\"\"\n",
    "SRC_TRAIN = r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\dataset\\csv\\ViT\\train_vit_new2old.csv\"\n",
    "SRC_TEST = r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\dataset\\csv\\ViT\\test_vit_new2old.csv\"\n",
    "\n",
    "SCALE_X = 1/2\n",
    "SCALE_Y = 1/2\n",
    "IMG_SIZE = 240\n",
    "\n",
    "class Dataset():\n",
    "  def __init__(self,scale_x = SCALE_X,scale_y = SCALE_Y,img_size=IMG_SIZE,src_train=SRC_TRAIN,src_test=SRC_TEST):\n",
    "    self.src_train = src_train\n",
    "    self.src_test = src_test\n",
    "    self.scale_x = scale_x\n",
    "    self.scale_y = scale_y\n",
    "    self.img_size = img_size\n",
    "\n",
    "    data_train,label_train,data_valid,label_valid,data_test,label_test = self.load_data()\n",
    "\n",
    "    img_train, label_train = self.url2img(data_train,label_train)\n",
    "    img_valid, label_valid = self.url2img(data_valid,label_valid)\n",
    "    img_test, label_test = self.url2img(data_test,label_test)\n",
    "\n",
    "    #when calculating (dividing process) the type have to be float\n",
    "    image_train = img_train.astype(\"float32\")\n",
    "    image_valid = img_valid.astype(\"float32\")\n",
    "    image_test = img_test.astype(\"float32\")\n",
    "\n",
    "    #when training, it is convenient if the values are normalized\n",
    "    #-1 ~ 1\n",
    "    img_train = img_train/255\n",
    "    img_train = 2*img_train - np.ones(img_train.shape)\n",
    "    img_valid = img_valid/255\n",
    "    img_valid = 2*img_valid - np.ones(img_valid.shape)\n",
    "    img_test = img_test/255\n",
    "    img_test = 2*img_test - np.ones(img_test.shape)\n",
    "\n",
    "    # converting data to float32, especially float32\n",
    "    self.img_train =np.expand_dims(np.asarray(img_train).astype(np.float32),1)\n",
    "    self.label_train = np.asarray(label_train).astype(np.int32)\n",
    "    self.img_valid = np.expand_dims(np.asarray(img_valid).astype(np.float32),1)\n",
    "    self.label_valid = np.asarray(label_valid).astype(np.int32)\n",
    "    self.img_test = np.expand_dims(np.asarray(img_test).astype(np.float32),1)\n",
    "    self.label_test = np.asarray(label_test).astype(np.int32)\n",
    "\n",
    "  def load_data(self):\n",
    "    file_train = pd.read_csv(filepath_or_buffer = self.src_train)\n",
    "    value_train = file_train.values\n",
    "    file_test = pd.read_csv(filepath_or_buffer = self.src_test)\n",
    "    value_test = file_test.values\n",
    "\n",
    "    print(value_test[:5])\n",
    "    ## ////////// テスト用とトレーニング用でデータをシャッフルする //////////////\n",
    "    #for 2D shuffle\n",
    "    value_train = shuffle(value_train,random_state=42) #random_state = integer ; fix randomness with shuffling\n",
    "    value_test = shuffle(value_test,random_state=42)\n",
    "\n",
    "    num_train = int(value_train.shape[0]//10*0.95)\n",
    "    #DATA,LABEL\n",
    "    #train,val\n",
    "    data_train = value_train[:num_train,1:]\n",
    "    label_train =  value_train[:num_train,0]\n",
    "    data_valid = value_train[num_train:int(value_train.shape[0]//10),1:]\n",
    "    label_valid =  value_train[num_train:int(value_train.shape[0]//10),0] #value_train.shape[0]//2\n",
    "    #test\n",
    "    data_test = value_test[:int(value_test.shape[0]//10),1:]\n",
    "    label_test = value_test[:int(value_test.shape[0]//10),0]\n",
    "\n",
    "    #check the distribution\n",
    "    count_test = 0\n",
    "    count_train = 0\n",
    "    count_valid = 0\n",
    "    for i in range(len(label_test)):\n",
    "      if label_test[i] == 1:\n",
    "        count_test += 1\n",
    "\n",
    "    for i in range(len(label_train)):\n",
    "      if label_train[i] == 1:\n",
    "        count_train += 1\n",
    "    \n",
    "    for i in range(len(label_valid)):\n",
    "      if label_train[i] == 1:\n",
    "        count_valid += 1\n",
    "\n",
    "    print('spatter label : : train : {}/{}, valid : {}/{} test : {}/{}'.format(count_train,len(label_train),count_valid,len(label_valid),count_test,len(label_test)))\n",
    "    return data_train,label_train,data_valid,label_valid,data_test,label_test\n",
    "\n",
    "\n",
    "  def url2img(self,data,label):\n",
    "    image_data = np.zeros((len(data),int(self.img_size*self.scale_x),int(self.img_size*self.scale_y)))\n",
    "    label_revised = []\n",
    "    size = (int(self.img_size*self.scale_x),int(self.img_size*self.scale_y))\n",
    "    for i,[name] in enumerate(data):\n",
    "      if i % 100 == 99:\n",
    "        print((len(data),i))\n",
    "      #print(name)\n",
    "      img=cv2.imread(name)\n",
    "      if np.any(img) == None:\n",
    "        continue\n",
    "      elif img[0,0,0] != None :\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.resize(gray,size,interpolation = cv2.INTER_AREA)\n",
    "        image_data[i] = gray\n",
    "        label_revised.append(label[i])\n",
    "    print(\"length of dataset is \",len(label_revised))\n",
    "    print(\"image data shape : \",image_data.shape)\n",
    "    imagedata = image_data[:len(label_revised)]\n",
    "    #imagedata = np.expand_dims(imagedata,1) #add color channel 1\n",
    "    return imagedata,label_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0\n",
      "  'C:\\\\Users\\\\Furukawa\\\\Documents\\\\TechAssistant\\\\dataset\\\\imgs\\\\20220401\\\\9_1\\\\concat\\\\0010.jpg']\n",
      " [0\n",
      "  'C:\\\\Users\\\\Furukawa\\\\Documents\\\\TechAssistant\\\\dataset\\\\imgs\\\\20220401\\\\9_1\\\\concat\\\\0012.jpg']\n",
      " [0\n",
      "  'C:\\\\Users\\\\Furukawa\\\\Documents\\\\TechAssistant\\\\dataset\\\\imgs\\\\20220401\\\\9_1\\\\concat\\\\0013.jpg']\n",
      " [0\n",
      "  'C:\\\\Users\\\\Furukawa\\\\Documents\\\\TechAssistant\\\\dataset\\\\imgs\\\\20220401\\\\9_1\\\\concat\\\\0014.jpg']\n",
      " [0\n",
      "  'C:\\\\Users\\\\Furukawa\\\\Documents\\\\TechAssistant\\\\dataset\\\\imgs\\\\20220401\\\\9_1\\\\concat\\\\0015.jpg']]\n",
      "spatter label : : train : 389/1044, valid : 18/55 test : 63/118\n",
      "(1044, 99)\n",
      "(1044, 199)\n",
      "(1044, 299)\n",
      "(1044, 399)\n",
      "(1044, 499)\n",
      "(1044, 599)\n",
      "(1044, 699)\n",
      "(1044, 799)\n",
      "(1044, 899)\n",
      "(1044, 999)\n",
      "length of dataset is  1044\n",
      "image data shape :  (1044, 120, 120)\n",
      "length of dataset is  55\n",
      "image data shape :  (55, 120, 120)\n",
      "(118, 99)\n",
      "length of dataset is  118\n",
      "image data shape :  (118, 120, 120)\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(scale_x = SCALE_X,scale_y = SCALE_Y,img_size=IMG_SIZE,src_train=SRC_TRAIN,src_test=SRC_TEST)\n",
    "img_train = dataset.img_train\n",
    "label_train = dataset.label_train\n",
    "img_valid = dataset.img_valid\n",
    "label_valid = dataset.label_valid\n",
    "img_test = dataset.img_test\n",
    "label_test = dataset.label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "    img_train,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    img_valid,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    img_test,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608]]],\n",
      "\n",
      "\n",
      "        [[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608]]],\n",
      "\n",
      "\n",
      "        [[[-0.9608, -0.9529, -0.9529,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9608, -0.9529, -0.9529,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9608, -0.9529, -0.9529,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608]]],\n",
      "\n",
      "\n",
      "        [[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9529, -0.9529, -0.9529]]],\n",
      "\n",
      "\n",
      "        [[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9608]]]])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x in train_dl:\n",
    "    if count == 1:\n",
    "        break\n",
    "    print(x)\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.59 GiB (GPU 0; 23.99 GiB total capacity; 67.52 GiB already allocated; 0 bytes free; 76.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m rec_img, mask \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     16\u001b[0m train_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((rec_img \u001b[39m-\u001b[39m x) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m mask) \u001b[39m/\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mmask_ratio\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m train_loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Furukawa\\anaconda3\\envs\\furukawa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[18], line 36\u001b[0m, in \u001b[0;36mMAE_ViT.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m---> 36\u001b[0m     features, backward_indexes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(img)\n\u001b[0;32m     37\u001b[0m     rec_img, mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(features, backward_indexes)\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m rec_img, mask\n",
      "File \u001b[1;32mc:\\Users\\Furukawa\\anaconda3\\envs\\furukawa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 62\u001b[0m, in \u001b[0;36mMAE_Encoder.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m patches \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token\u001b[39m.\u001b[39mrepeat(patches\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), patches], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[39m# 3. Encoderで入力データを処理する\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(patches))\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m features, backward_indexes\n",
      "File \u001b[1;32mc:\\Users\\Furukawa\\anaconda3\\envs\\furukawa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Furukawa\\anaconda3\\envs\\furukawa\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Furukawa\\anaconda3\\envs\\furukawa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x, return_attn)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, return_attn\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     27\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    x: (B, N, dim)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    B: バッチサイズ\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    N: 系列長\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    dim: 埋め込み次元\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     y, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_ln(x))\n\u001b[0;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m return_attn:  \u001b[39m# attention mapを返す（attention mapの可視化に利用）\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[39mreturn\u001b[39;00m attn\n",
      "File \u001b[1;32mc:\\Users\\Furukawa\\anaconda3\\envs\\furukawa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m v \u001b[39m=\u001b[39m rearrange(v, \u001b[39m\"\u001b[39m\u001b[39mb n (h d) -> b h n d\u001b[39m\u001b[39m\"\u001b[39m, h\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, d\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_head)\n\u001b[0;32m     60\u001b[0m \u001b[39m# QK^T / sqrt(d_k)を計算する\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m dots \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale\n\u001b[0;32m     64\u001b[0m \u001b[39m# ソフトマックス関数でスコアを算出し，Dropoutをする\u001b[39;00m\n\u001b[0;32m     65\u001b[0m attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattend(dots)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.59 GiB (GPU 0; 23.99 GiB total capacity; 67.52 GiB already allocated; 0 bytes free; 76.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # スケジューラで学習率を更新する\n",
    "    new_lr = scheduler(epoch)\n",
    "    set_lr(new_lr, optimizer)\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    total_valid_loss = 0.\n",
    "\n",
    "    # モデルの訓練\n",
    "    for x in train_dl:\n",
    "        step_count += 1\n",
    "        model.train()\n",
    "        x = x.to(device)\n",
    "\n",
    "        rec_img, mask = model(x)\n",
    "        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
    "        train_loss.backward()\n",
    "\n",
    "        if step_count % 8 == 0:  # 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "    # モデルの評価\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device)\n",
    "\n",
    "                rec_img, mask = model(x)\n",
    "                valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
    "\n",
    "                total_valid_loss += valid_loss.item()\n",
    "\n",
    "\n",
    "    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(train_dl):.4f} Valid Loss: {total_valid_loss/len(valid_dl):.4f}\")\n",
    "    if epoch % 30 == 29:\n",
    "        # モデルを保存しておく\n",
    "        torch.save(model.state_dict(), r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\ML_results\\vit\\MAE_pretrain_params_{epoch}.pth\".format(epoch=epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAE_ViT(**config).to(device)\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\ML_results\\vit\\MAE_pretrain_params_600.pth\", map_location=device))\n",
    "\n",
    "model.eval()\n",
    "x, _ = next(iter(valid_dl))\n",
    "with torch.no_grad():\n",
    "    rec_img, mask = model(x.to(device))\n",
    "\n",
    "x, rec_img, mask = x.to(\"cpu\"), rec_img.to(\"cpu\"), mask.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 15))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05,\n",
    "                    wspace=0.05)\n",
    "\n",
    "# MAEの出力そのままを可視化する場合\n",
    "# imgs = rec_img\n",
    "\n",
    "# マスクしていた部分は元の画像を用いる\n",
    "imgs = rec_img * mask + x * (1 - mask)\n",
    "imgs = (imgs.data + 1) / 2  # 標準化した状態から0-1に戻す\n",
    "i = 0\n",
    "for img in imgs[:64]:\n",
    "    # 出力が線形変換のため0-1になっているとは限らないためclipする\n",
    "    img = np.clip(np.transpose(torch.squeeze(img).numpy(), (1, 2, 0)), 0, 1) #(C,H,W) -> (H,W,C)\n",
    "    ax = fig.add_subplot(8, 8, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(img)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualize attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attn_map(model, x):\n",
    "    # Encoderの最後のattention mapを取得\n",
    "    attn = model.get_last_selfattention(x[0].unsqueeze(0).to(device))\n",
    "\n",
    "    # Nはパッチの数\n",
    "    # (1, num_head, N+1, N+1) -> (num_head, N)\n",
    "    num_head = config[\"enc_heads\"]\n",
    "    attn = attn[0, :, 0, 1:].reshape(num_head, -1)  # cls tokenに対するスコアを抽出\n",
    "\n",
    "    val, idx = torch.sort(attn)  # スコアを昇順でソート\n",
    "    val /= torch.sum(val, dim=1, keepdim=True)  # スコアを[0-1]で正規化する\n",
    "\n",
    "    # 累積和をとりスコアの合計が0.6ほどになるように残す\n",
    "    cumval = torch.cumsum(val, dim=1)\n",
    "    attn = cumval > (1 - 0.4)\n",
    "    backward_indexes = torch.argsort(idx)\n",
    "\n",
    "    # ソートしたものを戻す\n",
    "    for head in range(num_head):\n",
    "        attn[head] = attn[head][backward_indexes[head]]\n",
    "\n",
    "    # スコアを画像の形にする\n",
    "    w_featmap, h_featmap = config[\"image_size\"][0] // config[\"patch_size\"][0], config[\"image_size\"][1] // config[\"patch_size\"][1]\n",
    "    attn = attn.reshape(num_head, h_featmap, w_featmap).float()\n",
    "\n",
    "    # 入力画像と同じ大きさにする\n",
    "    attn = nn.functional.interpolate(attn.unsqueeze(0), scale_factor=config[\"patch_size\"][0], mode=\"nearest\")[0].detach().cpu().numpy()\n",
    "\n",
    "    # 入力画像とヘッドごとのattention mapを出力する\n",
    "    fig = plt.figure(figsize=(6, 10))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05,\n",
    "                        wspace=0.05)\n",
    "\n",
    "    img = (x[0].data + 1) / 2\n",
    "    img = np.clip(np.transpose(torch.squeeze(img).numpy(), (1, 2, 0)), 0, 1)\n",
    "    ax = fig.add_subplot(2, 3, 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for i in range(len(attn)):\n",
    "        featmap = attn[i]\n",
    "        featmap = np.concatenate((featmap[:,:,np.newaxis], np.zeros((32, 32, 2))), axis=2)\n",
    "        ax = fig.add_subplot(2, 3, i+4, xticks=[], yticks=[])\n",
    "        ax.imshow(img)\n",
    "        ax.imshow(featmap, alpha=0.5)\n",
    "\n",
    "\n",
    "model = MAE_ViT(**config).to(device)\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/model/MAE_pretrain_params.pth\", map_location=device))\n",
    "model.eval()\n",
    "x, _ = next(iter(valid_dl))  # 検証用データからデータを取得する\n",
    "\n",
    "display_attn_map(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ViT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_Classifier(nn.Module):\n",
    "    def __init__(self, encoder: MAE_Encoder, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = patches + self.pos_embedding  # positional embedding\n",
    "\n",
    "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        logits = self.head(features[:, 0])  # cls tokenのみを入力する\n",
    "        return logits\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        patches = self.patchify(x)\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)  # class tokenを結合\n",
    "        for i, block in enumerate(self.transformer):\n",
    "            if i < len(self.transformer) - 1:\n",
    "                patches = block(patches)\n",
    "            else:\n",
    "                return block(patches, return_attn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Visualization of attention map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdel\u001b[39;00m model, train_loss, valid_loss\n\u001b[0;32m      2\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "del model, train_loss, valid_loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = MAE_ViT(**config).to(device)\n",
    "encoder = mae.encoder\n",
    "\n",
    "# モデルの定義\n",
    "model = ViT_Classifier(encoder).to(device)\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\ML_results\\vit\\MAE_pretrain_params_200.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "display_attn_map(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "config = {\n",
    "    \"image_size\": [120, 120],\n",
    "    \"patch_size\": [3, 3],\n",
    "    \"emb_dim\": 192,\n",
    "    \"enc_layers\": 12,\n",
    "    \"enc_heads\": 3,\n",
    "    \"enc_dim_head\": 64,\n",
    "    \"enc_mlp_dim\": 192,\n",
    "    \"dec_layers\": 4,\n",
    "    \"dec_heads\": 3,\n",
    "    \"dec_dim_head\": 64,\n",
    "    \"dec_mlp_dim\": 192,\n",
    "    \"mask_ratio\": 1/9,\n",
    "    \"dropout\": 0.\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pretrained_model = MAE_ViT(**config).to(device)\n",
    "pretrained_model.load_state_dict(torch.load(r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\ML_results\\vit\\MAE_pretrain_params_600.pth\", map_location=device))\n",
    "\n",
    "encoder = pretrained_model.encoder\n",
    "\n",
    "# モデルの定義\n",
    "model = ViT_Classifier(encoder).to(device)\n",
    "\n",
    "epochs = 300\n",
    "lr = 0.0005\n",
    "warmup_length = 30\n",
    "batch_size = 64\n",
    "optimizer = optim.AdamW(model.head.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.05)  # 分類器部分のみ学習\n",
    "scheduler = CosineScheduler(epochs, lr, warmup_length)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH =int(IMG_SIZE * SCALE_X)\n",
    "HEIGHT = int(IMG_SIZE * SCALE_Y)\n",
    "\n",
    "class Process(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_train, t_train,width = WIDTH, height = HEIGHT):\n",
    "        self.x_train = x_train#.reshape(-1,1,width,height)#.astype('float32') / 255\n",
    "        self.t_train = t_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_train.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x_train[idx], dtype=torch.float), torch.tensor(self.t_train[idx], dtype=torch.long)\n",
    "\n",
    "train_data = Process(img_train, label_train)\n",
    "valid_data = Process(img_valid, label_valid)\n",
    "test_data = Process(img_test,label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(\n",
    "    valid_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 120, 120])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 1, 120, 120])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for t,x in dataloader_train:\n",
    "    print(t.shape)\n",
    "    print(x.shape)\n",
    "    if count == 1:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_count = 0\n",
    "for epoch in range(epochs):\n",
    "    new_lr = scheduler(epoch)\n",
    "    set_lr(new_lr, optimizer)\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    total_train_acc = 0.\n",
    "    total_valid_loss = 0.\n",
    "    total_valid_acc = 0.\n",
    "    for x, t in dataloader_train:\n",
    "        step_count += 1\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        pred = model(x)\n",
    "\n",
    "        train_loss = criterion(pred, t)\n",
    "        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += train_loss.item()\n",
    "        total_train_acc += train_acc\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, t in dataloader_valid:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            pred = model(x)\n",
    "\n",
    "            valid_loss = criterion(pred, t)\n",
    "            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n",
    "\n",
    "            total_valid_loss += valid_loss.item()\n",
    "            total_valid_acc += valid_acc\n",
    "\n",
    "    print(f\"Epoch[{epoch+1} / {epochs}]\",\n",
    "          f\"Train Loss: {total_train_loss/len(train_dl):.4f}\",\n",
    "          f\"Train Acc.: {total_train_acc/len(train_dl):.4f}\",\n",
    "          f\"Valid Loss: {total_valid_loss/len(valid_dl):.4f}\",\n",
    "          f\"Valid Acc.: {total_valid_acc/len(valid_dl):.4f}\")\n",
    "\n",
    "    if epoch %100 == 99:\n",
    "        torch.save(model.state_dict(), r\"C:\\Users\\Furukawa\\Documents\\TechAssistant\\ML_results\\vit\\vit_{}.pth\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    new_lr = scheduler(epoch)\n",
    "    set_lr(new_lr, optimizer)\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    total_train_acc = 0.\n",
    "    total_valid_loss = 0.\n",
    "    total_valid_acc = 0.\n",
    "    for x, t in train_dl:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        pred = model(x)\n",
    "\n",
    "        train_loss = criterion(pred, t)\n",
    "        train_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += train_loss.item()\n",
    "        total_train_acc += train_acc\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, t in valid_dl:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            pred = model(x)\n",
    "\n",
    "            valid_loss = criterion(pred, t)\n",
    "            valid_acc = (torch.argmax(pred, dim=1) == t).float().mean().cpu()\n",
    "\n",
    "            total_valid_loss += valid_loss.item()\n",
    "            total_valid_acc += valid_acc\n",
    "\n",
    "    print(f\"Epoch[{epoch+1} / {epochs}]\",\n",
    "          f\"Train Loss: {total_train_loss/len(train_dl):.4f}\",\n",
    "          f\"Train Acc.: {total_train_acc/len(train_dl):.4f}\",\n",
    "          f\"Valid Loss: {total_valid_loss/len(valid_dl):.4f}\",\n",
    "          f\"Valid Acc.: {total_valid_acc/len(valid_dl):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/DLBasics2023_colab/Lecture09/model/MAE_classifier_params.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "furukawa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
